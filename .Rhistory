head(GeoData)
# Check if any column in GeoData matches ID_REFA_LDA
table(GeoData$ZdCId %in% unique(dt$ID_REFA_LDA))
# Check duplicates in GeoData
GeoData_duplicates <- GeoData[duplicated(GeoData$ZdCId), ]
# Check duplicates in merged_data
merged_data_duplicates <- dt[duplicated(dt$ID_REFA_LDA), ]
# Select only the required columns
cols <- c("ZdAYEpsg2154", "ZdAXEpsg2154", "ZdCId")
GeoData <- GeoData[, cols, drop = FALSE]
head(GeoData)
# Select only duplicates in GeoData
GeoData_duplicates <- GeoData[duplicated(GeoData$ZdCId) | duplicated(GeoData$ZdCId, fromLast = TRUE), ]
# Display duplicates
head(GeoData_duplicates)
# Ensure ZdCId is unique in GeoData
GeoData_unique <- GeoData %>%
distinct(ZdCId, .keep_all = TRUE) # Keep only the first occurrence of each ZdCId
# Perform the join with dt, keeping only matching rows
final_data <- dt %>%
group_by(ID_REFA_LDA, JOUR, LIBELLE_ARRET) %>%
summarize(Sum_NB_VALD = sum(as.numeric(NB_VALD), na.rm = TRUE), .groups = 'drop') %>%
inner_join(GeoData_unique, by = c("ID_REFA_LDA" = "ZdCId")) # Use inner join to keep only common values
# View the resulting dataset
head(final_data)
nrow(final_data)
str(final_data)
# Count NA values for each column using apply
na_counts <- apply(final_data, 2, function(x) sum(is.na(x)))
print(na_counts)
fwrite(final_data, "final_data_cleaned.csv")
# Summary statistics for each column
summary(final_data)
# Check the structure of the dataset
str(final_data)
filtered_data <- final_data %>%
filter(!is.na(ZdAXEpsg2154) & !is.na(ZdAYEpsg2154) & !is.na(Sum_NB_VALD)) # Remove rows with missing values
#Convert to spatial object
filtered_data_sf <- st_as_sf(
filtered_data,
coords = c("ZdAXEpsg2154", "ZdAYEpsg2154"), # Longitude and Latitude columns
crs = 2154 # Coordinate Reference System: Lambert-93
)
filtered_data_sf <- st_transform(filtered_data_sf, crs = 4326)
#longitude and latitude for plotting
filtered_data <- filtered_data %>%
mutate(
X = st_coordinates(filtered_data_sf)[, "X"], # Longitude
Y = st_coordinates(filtered_data_sf)[, "Y"]  # Latitude
)
# a heatmap to visualize Sum_NB_VALD
my_heatmap <- leaflet(filtered_data) %>%
addTiles() %>% # Add OpenStreetMap tiles
setView(
lng = mean(filtered_data$X, na.rm = TRUE), # Center the map based on the average longitude
lat = mean(filtered_data$Y, na.rm = TRUE), # Center the map based on the average latitude
zoom = 11 # Set initial zoom level
) %>%
addHeatmap(
lng = ~X, # Use longitude for the heatmap
lat = ~Y, # Use latitude for the heatmap
intensity = ~Sum_NB_VALD, # Intensity based on Sum_NB_VALD
blur = 20, # Smoothness of heatmap points
max = max(filtered_data$Sum_NB_VALD, na.rm = TRUE), # Normalize intensity
radius = 15 # Size of heatmap points
)
#Display the heatmap
my_heatmap
# Aggregate total validations per day
daily_ridership <- final_data %>%
group_by(JOUR) %>%
summarize(Total_Validations = sum(Sum_NB_VALD, na.rm = TRUE), .groups = "drop")
# Plot daily ridership with yearly facets
ggplot(daily_ridership, aes(x = JOUR, y = Total_Validations)) +
geom_line(color = "blue") +
facet_wrap(~year(JOUR), scales = "free_x") +
labs(title = "Daily Ridership Trends by Year", x = "Date", y = "Total Validations") +
theme_minimal()
final_data <- final_data %>%
mutate(
Month = month(JOUR), # Extract numeric month
Month_Name = factor(month.name[Month], levels = month.name) # Convert to full month name
)
# Group by Month_Name for all months and calculate total ridership
monthly_trends <- final_data %>%
group_by(Month_Name) %>%
summarize(Total_Validations = sum(Sum_NB_VALD, na.rm = TRUE)) %>%
mutate(Month_Name = factor(Month_Name, levels = month.name)) # Order months
# Plot monthly ridership trends for all months
ggplot(monthly_trends, aes(x = Month_Name, y = Total_Validations)) +
geom_bar(stat = "identity", fill = "steelblue") +
labs(
title = "Monthly Ridership Trends for All Months",
x = "Month",
y = "Total Validations"
) +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) # Rotate x-axis labels for better readability
# Detect potential outliers based on IQR
daily_ridership <- final_data %>%
group_by(JOUR) %>%
summarize(Total_Validations = sum(Sum_NB_VALD, na.rm = TRUE))
Q1 <- quantile(daily_ridership$Total_Validations, 0.25, na.rm = TRUE)
Q3 <- quantile(daily_ridership$Total_Validations, 0.75, na.rm = TRUE)
IQR <- Q3 - Q1
outliers <- daily_ridership %>%
filter(Total_Validations < (Q1 - 1.5 * IQR) | Total_Validations > (Q3 + 1.5 * IQR))
# View outliers
print(outliers)
# Plot daily ridership highlighting outliers
ggplot(daily_ridership, aes(x = JOUR, y = Total_Validations)) +
geom_line(color = "blue") +
geom_point(data = outliers, aes(x = JOUR, y = Total_Validations), color = "red", size = 2) +
labs(title = "Daily Ridership with Outliers Highlighted", x = "Date", y = "Total Validations") +
theme_minimal()
# a list of major holidays in France from 2018 to 2023 (French names)
holiday_dates <- data.frame(
Date = as.Date(c(
"2018-01-01", "2018-05-01", "2018-07-14", "2018-12-25", # 2018
"2019-01-01", "2019-05-01", "2019-07-14", "2019-12-25", # 2019
"2020-01-01", "2020-05-01", "2020-07-14", "2020-12-25", # 2020
"2021-01-01", "2021-05-01", "2021-07-14", "2021-12-25", # 2021
"2022-01-01", "2022-05-01", "2022-07-14", "2022-12-25", # 2022
"2023-01-01", "2023-05-01", "2023-07-14", "2023-12-25"  # 2023
)),
Holiday = c(
"Jour de l'An", "Fête du Travail", "Fête Nationale", "Noël", # 2018
"Jour de l'An", "Fête du Travail", "Fête Nationale", "Noël", # 2019
"Jour de l'An", "Fête du Travail", "Fête Nationale", "Noël", # 2020
"Jour de l'An", "Fête du Travail", "Fête Nationale", "Noël", # 2021
"Jour de l'An", "Fête du Travail", "Fête Nationale", "Noël", # 2022
"Jour de l'An", "Fête du Travail", "Fête Nationale", "Noël"  # 2023
)
)
# Merge holiday information into final_data
final_data <- final_data %>%
left_join(holiday_dates, by = c("JOUR" = "Date")) %>%
mutate(Is_Holiday = ifelse(is.na(Holiday), "Jour non férié", Holiday)) # Assign holiday names or "Non-Holiday"
# Compare ridership by holiday status
holiday_effect <- final_data %>%
group_by(Is_Holiday) %>%
summarize(Average_Validations = mean(Sum_NB_VALD, na.rm = TRUE))
# Plot holiday vs. non-holiday ridership
ggplot(holiday_effect, aes(x = reorder(Is_Holiday, -Average_Validations), y = Average_Validations, fill = Is_Holiday)) +
geom_bar(stat = "identity") +
labs(
title = "Fréquentation pendant les jours fériés et non fériés",
x = "Jour férié",
y = "Validations moyennes"
) +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) # Rotate x-axis labels for readability
#  school vacation periods (example dates for France)
vacation_periods <- data.frame(
Start = as.Date(c("2020-02-15", "2020-04-10", "2020-07-01", "2020-12-19")),
End = as.Date(c("2020-03-02", "2020-04-27", "2020-08-31", "2021-01-03")),
Vacation = c("Winter Break", "Spring Break", "Summer Break", "Christmas Break")
)
# Add vacation labels to final_data
final_data <- final_data %>%
mutate(Vacation = case_when(
JOUR >= vacation_periods$Start[1] & JOUR <= vacation_periods$End[1] ~ "Winter Break",
JOUR >= vacation_periods$Start[2] & JOUR <= vacation_periods$End[2] ~ "Spring Break",
JOUR >= vacation_periods$Start[3] & JOUR <= vacation_periods$End[3] ~ "Summer Break",
JOUR >= vacation_periods$Start[4] & JOUR <= vacation_periods$End[4] ~ "Christmas Break",
TRUE ~ "No Vacation"
))
# Compare ridership during vacations vs. non-vacation periods
vacation_comparison <- final_data %>%
group_by(Vacation) %>%
summarize(Average_Validations = mean(Sum_NB_VALD, na.rm = TRUE))
# Plot vacation impact
ggplot(vacation_comparison, aes(x = Vacation, y = Average_Validations, fill = Vacation)) +
geom_bar(stat = "identity") +
labs(
title = "Impact of School Vacations on Ridership",
x = "Vacation Period",
y = "Average Validations"
) +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) # Rotate labels for readability
library(data.table)
library(dplyr)
library(stringr)
library(readxl)
library(shiny)
library(sf)
library(rsconnect)
library(leaflet)
library(ggplot2)
library(lubridate)
library(leaflet)
library(leaflet.extras)
data1 <- fread("data/2018_S1_NB_FER.txt", header = TRUE, sep = "\t")
library(data.table)
library(dplyr)
library(stringr)
library(readxl)
library(shiny)
library(sf)
library(rsconnect)
library(leaflet)
library(ggplot2)
library(lubridate)
library(leaflet)
library(leaflet.extras)
data1 <- fread("data/2018_S1_NB_FER.txt", header = TRUE, sep = "\t")
data2 <- fread("data/2018_S2_NB_FER.txt", header = TRUE, sep = "\t")
data3 <- fread("data/2019_S1_NB_FER.txt", header = TRUE, sep = "\t")
data4 <- fread("data/2019_S2_NB_FER.txt", header = TRUE, sep = "\t")
data5 <- fread("data/2020_S1_NB_FER.txt", header = TRUE, sep = "\t")
data6 <- fread("data/2020_S2_NB_FER.txt", header = TRUE, sep = "\t")
data7 <- fread("data/2021_S1_NB_FER.txt", header = TRUE, sep = "\t")
data8 <- fread("data/2021_S2_NB_FER.txt", header = TRUE, sep = "\t")
data9 <- fread("data/2022_S1_NB_FER.txt", header = TRUE, sep = "\t")
data10 <- fread("data/2022_S2_NB_FER.txt", header = TRUE, sep = ";")
# rm(list = ls())
data11<- fread("data/data-rf-2023-s1.csv", header = TRUE, sep = ";")
#combine the whole data
data_list <- list(data1, data2, data3, data4, data5, data6, data7, data8, data9, data10)
#print the column names and types
invisible(lapply(seq_along(data_list), function(i) {
cat("\n--- Structure of dataset", i, "---\n")
str(data_list[[i]])
}))
combined_dataI <- rbindlist(data_list)
# first few rows
head(data11)
# Get the column names
column_names <- names(data11)
print(column_names)
data11[[1]] <- as.Date(data11[[1]])  # Convert IDate to Date
# Verify the result
head(data11)
str(data11)
# Convert 'JOUR' column to Date format
combined_dataI$JOUR <- as.Date(combined_dataI$JOUR, format = "%d/%m/%Y")
# Verify the result
head(combined_dataI)
str(combined_dataI)
class(combined_dataI[[1]])
class(data11[[1]])
# Append data11 to the combined dataset
combined_data <- rbindlist(list(combined_dataI, data11))
# Verify the result
head(combined_data)
str(combined_data)
# Convert CODE_STIF_RES and CODE_STIF_ARRET to integer in place
combined_data[, CODE_STIF_RES := as.integer(CODE_STIF_RES)]
combined_data[, CODE_STIF_ARRET := as.integer(CODE_STIF_ARRET)]
#check for the converted datatypes
# Display structure in the console
str(combined_data)
# Identify numeric columns
numeric_cols <- names(combined_data)[sapply(combined_data, is.numeric)]
# Check for missing values in numeric columns
missing_summary <- sapply(numeric_cols, function(col) sum(is.na(combined_data[[col]])))
# Print the summary
print("Missing values in numeric columns:")
print(missing_summary)
#missing values percentage
missing_rate_res <- sum(is.na(combined_data$CODE_STIF_RES)) / nrow(combined_data)
missing_rate_arret <- sum(is.na(combined_data$CODE_STIF_ARRET)) / nrow(combined_data)
print(paste("Missing rate for CODE_STIF_RES:", missing_rate_res))
print(paste("Missing rate for CODE_STIF_ARRET:", missing_rate_arret))
# Remove rows with any NA values
dt<- na.omit(combined_data)
# Verify the number of rows before and after removal
nrow(combined_data)  # Original number of rows
nrow(dt)  # Number of rows after removal
# Convert the empty values to NA
dt[dt == ""] <- NA
# Replace "?" with NA in the column ID_REFA_LDA
dt<- dt %>%
mutate(ID_REFA_LDA = ifelse(ID_REFA_LDA == "?", NA, ID_REFA_LDA))
# Replace "?" with NA in the column CATEGORIE_TITRE
dt <- dt %>%
mutate(CATEGORIE_TITRE = ifelse(CATEGORIE_TITRE == "?", NA, CATEGORIE_TITRE))
# Count the number of missing values for each column
missing_values <- colSums(is.na(dt))
# Display results
print(missing_values)
# Delete missing values
dt <- na.omit(dt)
# Count the number of missing values in every column
missing_values <- colSums(is.na(dt))
# Display the result
print(missing_values)
dt <-dt[!duplicated(dt), ]
nrow(dt)
boxplot(dt$NB_VALD)
# Extract the NB_VALD column
nb <- dt$NB_VALD
# Calculate Q1 (25th percentile) and Q3 (75th percentile)
Q1 <- quantile(nb, 0.25, na.rm = TRUE)
Q3 <- quantile(nb, 0.75, na.rm = TRUE)
# Calculate IQR
IQR <- Q3 - Q1
# Define lower and upper bounds for outliers
lower_bound <- Q1 - 1.5 * IQR
upper_bound <- Q3 + 1.5 * IQR
# Identify and print outliers
outliers <- nb[nb < lower_bound | nb > upper_bound]
cat("Outliers:\n")
print(outliers)
nrow(dt)
head(dt)
# Load the new dataset
# Read the CSV file with a header
GeoData <- read.csv("data/zones-d-arrets.csv", header = TRUE, sep = ";")
# Display the first few rows
str(GeoData)
str(GeoData)
# Columns to check
columns_to_check <- c("ZdAYEpsg2154", "ZdAXEpsg2154", "ZdCId")
# Check for NA values in these columns
na_counts <- sapply(GeoData[columns_to_check], function(col) sum(is.na(col)))
# Print the count of NA values for each column
print(na_counts)
GeoData <-GeoData[!duplicated(GeoData), ]
head(GeoData)
# Check if any column in GeoData matches ID_REFA_LDA
table(GeoData$ZdCId %in% unique(dt$ID_REFA_LDA))
# Check duplicates in GeoData
GeoData_duplicates <- GeoData[duplicated(GeoData$ZdCId), ]
# Check duplicates in merged_data
merged_data_duplicates <- dt[duplicated(dt$ID_REFA_LDA), ]
# Select only the required columns
cols <- c("ZdAYEpsg2154", "ZdAXEpsg2154", "ZdCId")
GeoData <- GeoData[, cols, drop = FALSE]
head(GeoData)
# Select only duplicates in GeoData
GeoData_duplicates <- GeoData[duplicated(GeoData$ZdCId) | duplicated(GeoData$ZdCId, fromLast = TRUE), ]
# Display duplicates
head(GeoData_duplicates)
# Ensure ZdCId is unique in GeoData
GeoData_unique <- GeoData %>%
distinct(ZdCId, .keep_all = TRUE) # Keep only the first occurrence of each ZdCId
# Perform the join with dt, keeping only matching rows
final_data <- dt %>%
group_by(ID_REFA_LDA, JOUR, LIBELLE_ARRET) %>%
summarize(Sum_NB_VALD = sum(as.numeric(NB_VALD), na.rm = TRUE), .groups = 'drop') %>%
inner_join(GeoData_unique, by = c("ID_REFA_LDA" = "ZdCId")) # Use inner join to keep only common values
# View the resulting dataset
head(final_data)
nrow(final_data)
str(final_data)
# Count NA values for each column using apply
na_counts <- apply(final_data, 2, function(x) sum(is.na(x)))
print(na_counts)
fwrite(final_data, "final_data_cleaned.csv")
# Summary statistics for each column
summary(final_data)
# Check the structure of the dataset
str(final_data)
filtered_data <- final_data %>%
filter(!is.na(ZdAXEpsg2154) & !is.na(ZdAYEpsg2154) & !is.na(Sum_NB_VALD)) # Remove rows with missing values
#Convert to spatial object
filtered_data_sf <- st_as_sf(
filtered_data,
coords = c("ZdAXEpsg2154", "ZdAYEpsg2154"), # Longitude and Latitude columns
crs = 2154 # Coordinate Reference System: Lambert-93
)
filtered_data_sf <- st_transform(filtered_data_sf, crs = 4326)
#longitude and latitude for plotting
filtered_data <- filtered_data %>%
mutate(
X = st_coordinates(filtered_data_sf)[, "X"], # Longitude
Y = st_coordinates(filtered_data_sf)[, "Y"]  # Latitude
)
# a heatmap to visualize Sum_NB_VALD
my_heatmap <- leaflet(filtered_data) %>%
addTiles() %>% # Add OpenStreetMap tiles
setView(
lng = mean(filtered_data$X, na.rm = TRUE), # Center the map based on the average longitude
lat = mean(filtered_data$Y, na.rm = TRUE), # Center the map based on the average latitude
zoom = 11 # Set initial zoom level
) %>%
addHeatmap(
lng = ~X, # Use longitude for the heatmap
lat = ~Y, # Use latitude for the heatmap
intensity = ~Sum_NB_VALD, # Intensity based on Sum_NB_VALD
blur = 20, # Smoothness of heatmap points
max = max(filtered_data$Sum_NB_VALD, na.rm = TRUE), # Normalize intensity
radius = 15 # Size of heatmap points
)
#Display the heatmap
my_heatmap
# Aggregate total validations per day
daily_ridership <- final_data %>%
group_by(JOUR) %>%
summarize(Total_Validations = sum(Sum_NB_VALD, na.rm = TRUE), .groups = "drop")
# Plot daily ridership with yearly facets
ggplot(daily_ridership, aes(x = JOUR, y = Total_Validations)) +
geom_line(color = "blue") +
facet_wrap(~year(JOUR), scales = "free_x") +
labs(title = "Daily Ridership Trends by Year", x = "Date", y = "Total Validations") +
theme_minimal()
final_data <- final_data %>%
mutate(
Month = month(JOUR), # Extract numeric month
Month_Name = factor(month.name[Month], levels = month.name) # Convert to full month name
)
# Group by Month_Name for all months and calculate total ridership
monthly_trends <- final_data %>%
group_by(Month_Name) %>%
summarize(Total_Validations = sum(Sum_NB_VALD, na.rm = TRUE)) %>%
mutate(Month_Name = factor(Month_Name, levels = month.name)) # Order months
# Plot monthly ridership trends for all months
ggplot(monthly_trends, aes(x = Month_Name, y = Total_Validations)) +
geom_bar(stat = "identity", fill = "steelblue") +
labs(
title = "Monthly Ridership Trends for All Months",
x = "Month",
y = "Total Validations"
) +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) # Rotate x-axis labels for better readability
# Detect potential outliers based on IQR
daily_ridership <- final_data %>%
group_by(JOUR) %>%
summarize(Total_Validations = sum(Sum_NB_VALD, na.rm = TRUE))
Q1 <- quantile(daily_ridership$Total_Validations, 0.25, na.rm = TRUE)
Q3 <- quantile(daily_ridership$Total_Validations, 0.75, na.rm = TRUE)
IQR <- Q3 - Q1
outliers <- daily_ridership %>%
filter(Total_Validations < (Q1 - 1.5 * IQR) | Total_Validations > (Q3 + 1.5 * IQR))
# View outliers
print(outliers)
# Plot daily ridership highlighting outliers
ggplot(daily_ridership, aes(x = JOUR, y = Total_Validations)) +
geom_line(color = "blue") +
geom_point(data = outliers, aes(x = JOUR, y = Total_Validations), color = "red", size = 2) +
labs(title = "Daily Ridership with Outliers Highlighted", x = "Date", y = "Total Validations") +
theme_minimal()
# a list of major holidays in France from 2018 to 2023 (French names)
holiday_dates <- data.frame(
Date = as.Date(c(
"2018-01-01", "2018-05-01", "2018-07-14", "2018-12-25", # 2018
"2019-01-01", "2019-05-01", "2019-07-14", "2019-12-25", # 2019
"2020-01-01", "2020-05-01", "2020-07-14", "2020-12-25", # 2020
"2021-01-01", "2021-05-01", "2021-07-14", "2021-12-25", # 2021
"2022-01-01", "2022-05-01", "2022-07-14", "2022-12-25", # 2022
"2023-01-01", "2023-05-01", "2023-07-14", "2023-12-25"  # 2023
)),
Holiday = c(
"Jour de l'An", "Fête du Travail", "Fête Nationale", "Noël", # 2018
"Jour de l'An", "Fête du Travail", "Fête Nationale", "Noël", # 2019
"Jour de l'An", "Fête du Travail", "Fête Nationale", "Noël", # 2020
"Jour de l'An", "Fête du Travail", "Fête Nationale", "Noël", # 2021
"Jour de l'An", "Fête du Travail", "Fête Nationale", "Noël", # 2022
"Jour de l'An", "Fête du Travail", "Fête Nationale", "Noël"  # 2023
)
)
# Merge holiday information into final_data
final_data <- final_data %>%
left_join(holiday_dates, by = c("JOUR" = "Date")) %>%
mutate(Is_Holiday = ifelse(is.na(Holiday), "Jour non férié", Holiday)) # Assign holiday names or "Non-Holiday"
# Compare ridership by holiday status
holiday_effect <- final_data %>%
group_by(Is_Holiday) %>%
summarize(Average_Validations = mean(Sum_NB_VALD, na.rm = TRUE))
# Plot holiday vs. non-holiday ridership
ggplot(holiday_effect, aes(x = reorder(Is_Holiday, -Average_Validations), y = Average_Validations, fill = Is_Holiday)) +
geom_bar(stat = "identity") +
labs(
title = "Fréquentation pendant les jours fériés et non fériés",
x = "Jour férié",
y = "Validations moyennes"
) +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) # Rotate x-axis labels for readability
#  school vacation periods (example dates for France)
vacation_periods <- data.frame(
Start = as.Date(c("2020-02-15", "2020-04-10", "2020-07-01", "2020-12-19")),
End = as.Date(c("2020-03-02", "2020-04-27", "2020-08-31", "2021-01-03")),
Vacation = c("Winter Break", "Spring Break", "Summer Break", "Christmas Break")
)
# Add vacation labels to final_data
final_data <- final_data %>%
mutate(Vacation = case_when(
JOUR >= vacation_periods$Start[1] & JOUR <= vacation_periods$End[1] ~ "Winter Break",
JOUR >= vacation_periods$Start[2] & JOUR <= vacation_periods$End[2] ~ "Spring Break",
JOUR >= vacation_periods$Start[3] & JOUR <= vacation_periods$End[3] ~ "Summer Break",
JOUR >= vacation_periods$Start[4] & JOUR <= vacation_periods$End[4] ~ "Christmas Break",
TRUE ~ "No Vacation"
))
# Compare ridership during vacations vs. non-vacation periods
vacation_comparison <- final_data %>%
group_by(Vacation) %>%
summarize(Average_Validations = mean(Sum_NB_VALD, na.rm = TRUE))
# Plot vacation impact
ggplot(vacation_comparison, aes(x = Vacation, y = Average_Validations, fill = Vacation)) +
geom_bar(stat = "identity") +
labs(
title = "Impact of School Vacations on Ridership",
x = "Vacation Period",
y = "Average Validations"
) +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) # Rotate labels for readability
# Shapiro-Wilk test for normality
shapiro_result <- shapiro.test(sample(data$Sum_NB_VALD, 5000, replace = FALSE))
shapiro_stat <- shapiro_result$statistic
p_value_shapiro <- shapiro_result$p.value
# Display results
print(paste("Shapiro-Wilk Test Statistic:", shapiro_stat))
print(paste("P-value:", p_value_shapiro))
kruskal_result <- kruskal.test(Sum_NB_VALD ~ LIBELLE_ARRET, data = data)
print(kruskal_result)
kruskal_result <- kruskal.test(Sum_NB_VALD ~ LIBELLE_ARRET, data = data)
print(kruskal_result)
